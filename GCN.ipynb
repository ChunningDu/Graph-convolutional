{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_batches\n",
    "from data import prepare_data\n",
    "from data import data_to_index\n",
    "from data import DEP_LABELS\n",
    "\n",
    "from model.graph import Sintactic_GCN\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder_luong\n",
    "\n",
    "from BLEU import BLEU\n",
    "\n",
    "from utils import time_since\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from validation import Evaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "SPLIT_TRAIN = 0.7\n",
    "SPLIT_VALID = 0.15\n",
    "# The rest is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare vocabulary and pairs for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 118964 sentence pairs\n",
      "Filtered to 85785 pairs\n",
      "Creating vocab...\n",
      "Indexed 12436 words in input language, 22765 words in output\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('en', 'spa', max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting pairs into test, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shuffle(pairs)\n",
    "pairs_train = pairs[:int(len(pairs) * SPLIT_TRAIN)]\n",
    "pairs_valid = pairs[int(len(pairs) * SPLIT_TRAIN):int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID))]\n",
    "pairs_test = pairs[int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60049, 12868, 12868)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_train), len(pairs_valid), len(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the adjacency matrix for the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'/home/krivas/stanford-corenlp-full-2018-02-27/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_adjacency_matrix(pairs):\n",
    "    arr_dep = []\n",
    "    for pair in tqdm(pairs):\n",
    "        arr_dep.append(nlp.dependency_parse(pair[0]))\n",
    "    return np.array(arr_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60049/60049 [07:20<00:00, 136.33it/s]\n",
      "100%|██████████| 12868/12868 [02:03<00:00, 104.42it/s]\n",
      "100%|██████████| 12868/12868 [02:29<00:00, 86.21it/s]\n"
     ]
    }
   ],
   "source": [
    "arr_dep_train = get_adjacency_matrix(pairs_train)\n",
    "arr_dep_valid = get_adjacency_matrix(pairs_valid)\n",
    "arr_dep_test = get_adjacency_matrix(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting words to index in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train = data_to_index(pairs_train, input_lang, output_lang)\n",
    "pairs_valid = data_to_index(pairs_valid, input_lang, output_lang)\n",
    "pairs_test = data_to_index(pairs_test, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_batch_luong(batch_size, input_batches, target_batches, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "        \n",
    "    hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([input_lang.vocab.stoi[\"<sos>\"]] * batch_size))\n",
    "    \n",
    "    if gcn1:\n",
    "        encoder_outputs = gcn1(encoder_outputs,\n",
    "                             adj_arc_in, adj_arc_out,\n",
    "                             adj_lab_in, adj_lab_out,\n",
    "                             mask_in, mask_out,  \n",
    "                             mask_loop)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size)) \n",
    "    \n",
    "    all_decoder_outputs = Variable(torch.zeros(target_batches.data.size()[0], batch_size, len(output_lang.vocab.itos)))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    if train:\n",
    "        use_teacher_forcing = np.random.random() < tf_ratio\n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "    \n",
    "    if use_teacher_forcing:        \n",
    "        # Use targets as inputs\n",
    "        for di in range(target_batches.shape[0]):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            decoder_input = target_batches[di]\n",
    "    else:        \n",
    "        # Use decoder output as inputs\n",
    "        for di in range(target_batches.shape[0]):            \n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs) \n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            \n",
    "            # Greedy approach, take the word with highest probability\n",
    "            topv, topi = decoder_output.data.topk(1)            \n",
    "            decoder_input = Variable(torch.LongTensor(topi.cpu()).squeeze())\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "        \n",
    "    del decoder_output\n",
    "    del decoder_hidden\n",
    "        \n",
    "    return all_decoder_outputs, target_batches\n",
    "\n",
    "def train_luong(input_batches, target_batches, batch_size, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    if train:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0 # Added onto for each word\n",
    "    all_decoder_outputs, target_batches = pass_batch_luong(batch_size, input_batches, target_batches, train, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "    \n",
    "    # Loss calculation and backpropagation\n",
    "    loss = criterion(all_decoder_outputs.view(-1, decoder.output_size), target_batches.contiguous().view(-1))\n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        if gcn1:\n",
    "            torch.nn.utils.clip_grad_norm_(gcn1.parameters(), clip)\n",
    "            gcn1_optimizer.step()\n",
    "\n",
    "    del all_decoder_outputs\n",
    "    del target_batches\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure models\n",
    "hidden_size_rnn = 512\n",
    "hidden_size_graph = 512\n",
    "emb_size=300\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 10.0\n",
    "learning_rate_graph = 0.0002\n",
    "n_epochs = 20\n",
    "print_every = 10\n",
    "validate_loss_every = 50\n",
    "validate_acc_every = 2 * validate_loss_every\n",
    "tf_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = Encoder(len(input_lang.vocab.itos), hidden_size_rnn, emb_size, n_layers=n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "decoder = Decoder_luong('general', hidden_size_graph, len(output_lang.vocab.itos), 300, n_layers=2 * n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "gcn1 = Sintactic_GCN(hidden_size_rnn, hidden_size_graph, num_labels=len(DEP_LABELS))\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "gcn1_optimizer = optim.Adam(gcn1.parameters())#, learning_rate_graph)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    gcn1 = gcn1.cuda()\n",
    "    \n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "validation_bleu = []\n",
    "\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 15s (- 2412m 41s) (10 0.83%) train_loss: 1.2203\n",
      "20m 17s (- 1198m 35s) (20 1.67%) train_loss: 1.1139\n",
      "20m 21s (- 794m 39s) (30 2.50%) train_loss: 1.1144\n",
      "20m 24s (- 592m 32s) (40 3.33%) train_loss: 1.0043\n",
      "20m 28s (- 471m 13s) (50 4.16%) train_loss: 1.0333\n",
      "20m 31s (- 390m 21s) (60 5.00%) train_loss: 1.1199\n",
      "20m 35s (- 332m 36s) (70 5.83%) train_loss: 1.0522\n",
      "20m 38s (- 289m 15s) (80 6.66%) train_loss: 1.1035\n",
      "20m 42s (- 255m 33s) (90 7.49%) train_loss: 1.0058\n",
      "20m 45s (- 228m 32s) (100 8.33%) train_loss: 0.9728\n",
      "20m 48s (- 206m 24s) (110 9.16%) train_loss: 1.0389\n",
      "20m 51s (- 187m 57s) (120 9.99%) train_loss: 0.9836\n",
      "20m 55s (- 172m 22s) (130 10.82%) train_loss: 1.0444\n",
      "20m 58s (- 159m 0s) (140 11.66%) train_loss: 1.0992\n",
      "21m 2s (- 147m 25s) (150 12.49%) train_loss: 1.0344\n",
      "21m 5s (- 137m 16s) (160 13.32%) train_loss: 1.0982\n",
      "21m 9s (- 128m 17s) (170 14.15%) train_loss: 0.9781\n",
      "21m 12s (- 120m 19s) (180 14.99%) train_loss: 1.1101\n",
      "21m 16s (- 113m 10s) (190 15.82%) train_loss: 0.9324\n",
      "21m 19s (- 106m 43s) (200 16.65%) train_loss: 1.0438\n",
      "21m 22s (- 100m 53s) (210 17.49%) train_loss: 1.0712\n",
      "21m 26s (- 95m 35s) (220 18.32%) train_loss: 1.0208\n",
      "21m 29s (- 90m 44s) (230 19.15%) train_loss: 1.0138\n",
      "21m 32s (- 86m 16s) (240 19.98%) train_loss: 0.9387\n",
      "21m 36s (- 82m 11s) (250 20.82%) train_loss: 0.9820\n",
      "21m 40s (- 78m 25s) (260 21.65%) train_loss: 1.0797\n",
      "21m 43s (- 74m 55s) (270 22.48%) train_loss: 1.0099\n",
      "21m 46s (- 71m 38s) (280 23.31%) train_loss: 1.0213\n",
      "21m 50s (- 68m 37s) (290 24.15%) train_loss: 0.9459\n",
      "21m 53s (- 65m 46s) (300 24.98%) train_loss: 0.9550\n",
      "21m 57s (- 63m 6s) (310 25.81%) train_loss: 0.9619\n",
      "22m 0s (- 60m 36s) (320 26.64%) train_loss: 1.0049\n",
      "22m 4s (- 58m 15s) (330 27.48%) train_loss: 0.9336\n",
      "22m 7s (- 56m 2s) (340 28.31%) train_loss: 1.0893\n",
      "22m 11s (- 53m 57s) (350 29.14%) train_loss: 0.9320\n",
      "22m 14s (- 51m 58s) (360 29.98%) train_loss: 0.9727\n",
      "22m 18s (- 50m 6s) (370 30.81%) train_loss: 1.0794\n",
      "22m 22s (- 48m 19s) (380 31.64%) train_loss: 1.0025\n",
      "22m 25s (- 46m 38s) (390 32.47%) train_loss: 1.0497\n",
      "22m 28s (- 45m 1s) (400 33.31%) train_loss: 0.9208\n",
      "22m 32s (- 43m 29s) (410 34.14%) train_loss: 1.0832\n",
      "22m 35s (- 42m 0s) (420 34.97%) train_loss: 1.0156\n",
      "22m 39s (- 40m 36s) (430 35.80%) train_loss: 0.9661\n",
      "22m 42s (- 39m 16s) (440 36.64%) train_loss: 0.9601\n",
      "22m 46s (- 38m 0s) (450 37.47%) train_loss: 1.1125\n",
      "22m 50s (- 36m 47s) (460 38.30%) train_loss: 0.9686\n",
      "22m 53s (- 35m 36s) (470 39.13%) train_loss: 0.9223\n",
      "22m 57s (- 34m 28s) (480 39.97%) train_loss: 1.0128\n",
      "23m 0s (- 33m 23s) (490 40.80%) train_loss: 0.9751\n",
      "23m 4s (- 32m 20s) (500 41.63%) train_loss: 0.8885\n",
      "23m 7s (- 31m 20s) (510 42.46%) train_loss: 0.9653\n",
      "23m 11s (- 30m 22s) (520 43.30%) train_loss: 1.0254\n",
      "23m 14s (- 29m 25s) (530 44.13%) train_loss: 0.9552\n",
      "23m 18s (- 28m 31s) (540 44.96%) train_loss: 0.9351\n",
      "23m 21s (- 27m 39s) (550 45.80%) train_loss: 1.0553\n",
      "23m 25s (- 26m 48s) (560 46.63%) train_loss: 1.1461\n",
      "23m 28s (- 25m 59s) (570 47.46%) train_loss: 1.0197\n",
      "23m 31s (- 25m 11s) (580 48.29%) train_loss: 1.0782\n",
      "23m 35s (- 24m 25s) (590 49.13%) train_loss: 1.0571\n",
      "23m 38s (- 23m 41s) (600 49.96%) train_loss: 1.0685\n",
      "23m 42s (- 22m 57s) (610 50.79%) train_loss: 0.9897\n",
      "23m 45s (- 22m 15s) (620 51.62%) train_loss: 0.9331\n",
      "23m 48s (- 21m 34s) (630 52.46%) train_loss: 1.0109\n",
      "23m 51s (- 20m 55s) (640 53.29%) train_loss: 0.9991\n",
      "23m 55s (- 20m 16s) (650 54.12%) train_loss: 1.0245\n",
      "23m 58s (- 19m 39s) (660 54.95%) train_loss: 0.9244\n",
      "24m 1s (- 19m 2s) (670 55.79%) train_loss: 0.9143\n",
      "24m 4s (- 18m 27s) (680 56.62%) train_loss: 0.9991\n",
      "24m 8s (- 17m 52s) (690 57.45%) train_loss: 1.0070\n",
      "24m 11s (- 17m 18s) (700 58.28%) train_loss: 1.0192\n",
      "24m 14s (- 16m 46s) (710 59.12%) train_loss: 0.9989\n",
      "24m 18s (- 16m 14s) (720 59.95%) train_loss: 0.9248\n",
      "24m 21s (- 15m 42s) (730 60.78%) train_loss: 0.9392\n",
      "24m 24s (- 15m 12s) (740 61.62%) train_loss: 0.9913\n",
      "24m 27s (- 14m 42s) (750 62.45%) train_loss: 1.0114\n",
      "24m 31s (- 14m 13s) (760 63.28%) train_loss: 0.9836\n",
      "24m 34s (- 13m 45s) (770 64.11%) train_loss: 1.0256\n",
      "24m 37s (- 13m 17s) (780 64.95%) train_loss: 0.9418\n",
      "24m 40s (- 12m 50s) (790 65.78%) train_loss: 0.8622\n",
      "24m 43s (- 12m 23s) (800 66.61%) train_loss: 0.9120\n",
      "24m 47s (- 11m 57s) (810 67.44%) train_loss: 0.9066\n",
      "24m 50s (- 11m 32s) (820 68.28%) train_loss: 0.8626\n",
      "24m 53s (- 11m 7s) (830 69.11%) train_loss: 0.9481\n",
      "24m 56s (- 10m 43s) (840 69.94%) train_loss: 0.9916\n",
      "24m 59s (- 10m 19s) (850 70.77%) train_loss: 0.9266\n",
      "25m 3s (- 9m 56s) (860 71.61%) train_loss: 0.8195\n",
      "25m 6s (- 9m 33s) (870 72.44%) train_loss: 1.0819\n",
      "25m 9s (- 9m 10s) (880 73.27%) train_loss: 0.9594\n",
      "25m 12s (- 8m 48s) (890 74.10%) train_loss: 0.9820\n",
      "25m 15s (- 8m 27s) (900 74.94%) train_loss: 0.8607\n",
      "25m 19s (- 8m 5s) (910 75.77%) train_loss: 0.9796\n",
      "25m 22s (- 7m 44s) (920 76.60%) train_loss: 0.8836\n",
      "25m 25s (- 7m 24s) (930 77.44%) train_loss: 0.9942\n",
      "25m 28s (- 7m 4s) (940 78.27%) train_loss: 0.9344\n",
      "25m 32s (- 6m 44s) (950 79.10%) train_loss: 0.9348\n",
      "25m 35s (- 6m 25s) (960 79.93%) train_loss: 0.9251\n",
      "25m 39s (- 6m 6s) (970 80.77%) train_loss: 0.9450\n",
      "25m 42s (- 5m 47s) (980 81.60%) train_loss: 0.8237\n",
      "25m 46s (- 5m 29s) (990 82.43%) train_loss: 0.9791\n",
      "25m 49s (- 5m 11s) (1000 83.26%) train_loss: 0.8828\n",
      "25m 52s (- 4m 53s) (1010 84.10%) train_loss: 0.9165\n",
      "25m 56s (- 4m 36s) (1020 84.93%) train_loss: 0.8292\n",
      "25m 58s (- 4m 18s) (1030 85.76%) train_loss: 0.8843\n",
      "26m 1s (- 4m 1s) (1040 86.59%) train_loss: 0.9126\n",
      "26m 4s (- 3m 44s) (1050 87.43%) train_loss: 0.9069\n",
      "26m 6s (- 3m 28s) (1060 88.26%) train_loss: 0.9244\n",
      "26m 9s (- 3m 12s) (1070 89.09%) train_loss: 0.8285\n",
      "26m 12s (- 2m 56s) (1080 89.93%) train_loss: 0.9147\n",
      "26m 14s (- 2m 40s) (1090 90.76%) train_loss: 0.9457\n",
      "26m 17s (- 2m 24s) (1100 91.59%) train_loss: 0.8916\n",
      "26m 20s (- 2m 9s) (1110 92.42%) train_loss: 0.9570\n",
      "26m 22s (- 1m 54s) (1120 93.26%) train_loss: 0.9510\n",
      "26m 24s (- 1m 39s) (1130 94.09%) train_loss: 0.8888\n",
      "26m 27s (- 1m 24s) (1140 94.92%) train_loss: 0.8760\n",
      "26m 29s (- 1m 10s) (1150 95.75%) train_loss: 0.9876\n",
      "26m 32s (- 0m 56s) (1160 96.59%) train_loss: 0.9224\n",
      "26m 34s (- 0m 42s) (1170 97.42%) train_loss: 0.8932\n",
      "26m 36s (- 0m 28s) (1180 98.25%) train_loss: 0.8647\n",
      "26m 39s (- 0m 14s) (1190 99.08%) train_loss: 0.9211\n",
      "26m 41s (- 0m 1s) (1200 99.92%) train_loss: 1.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12867/12867 [06:54<00:00, 31.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 3.1782 - bleu: (0.026838692186651622, [0.39281976611912456, 0.07529530602568596, 0.019512542928782784, 0.0051203636159587], 0.6473175354188497) 34m 8s (- 4066m 2s) (10 0.83%) train_loss: 0.8135\n",
      "34m 12s (- 2019m 31s) (20 1.67%) train_loss: 1.5865\n",
      "34m 15s (- 1337m 19s) (30 2.50%) train_loss: 0.7146\n",
      "34m 19s (- 996m 12s) (40 3.33%) train_loss: 0.6166\n",
      "34m 23s (- 791m 31s) (50 4.16%) train_loss: 0.7948\n",
      "34m 26s (- 655m 2s) (60 5.00%) train_loss: 0.6737\n",
      "34m 30s (- 557m 34s) (70 5.83%) train_loss: 0.7570\n",
      "34m 34s (- 484m 24s) (80 6.66%) train_loss: 0.6676\n",
      "34m 37s (- 427m 29s) (90 7.49%) train_loss: 0.6885\n",
      "34m 41s (- 381m 56s) (100 8.33%) train_loss: 0.7126\n",
      "34m 45s (- 344m 39s) (110 9.16%) train_loss: 0.7436\n",
      "34m 48s (- 313m 35s) (120 9.99%) train_loss: 0.6800\n",
      "34m 52s (- 287m 17s) (130 10.82%) train_loss: 0.7200\n",
      "34m 56s (- 264m 45s) (140 11.66%) train_loss: 0.6598\n",
      "34m 59s (- 245m 12s) (150 12.49%) train_loss: 0.7567\n",
      "35m 3s (- 228m 5s) (160 13.32%) train_loss: 0.7362\n",
      "35m 7s (- 212m 59s) (170 14.15%) train_loss: 0.8127\n",
      "35m 10s (- 199m 33s) (180 14.99%) train_loss: 0.7375\n",
      "35m 14s (- 187m 32s) (190 15.82%) train_loss: 0.7317\n",
      "35m 18s (- 176m 41s) (200 16.65%) train_loss: 0.6555\n",
      "35m 21s (- 166m 53s) (210 17.49%) train_loss: 0.6892\n",
      "35m 25s (- 157m 58s) (220 18.32%) train_loss: 0.7519\n",
      "35m 29s (- 149m 49s) (230 19.15%) train_loss: 0.6520\n",
      "35m 33s (- 142m 21s) (240 19.98%) train_loss: 0.7134\n",
      "35m 36s (- 135m 28s) (250 20.82%) train_loss: 0.7269\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs): \n",
    "    # Shuffle data\n",
    "    id_aux = np.random.permutation(np.arange(len(pairs_train)))\n",
    "    pairs_train = pairs_train[id_aux]\n",
    "    arr_dep_train = arr_dep_train[id_aux]\n",
    "    \n",
    "    # Get the batches for this epoch\n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_train, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for batch_ix, (input_batch, target_var) in enumerate(zip(input_batches, target_batches)):\n",
    "    \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        gcn1.train()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                    True, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if batch_ix == 0: continue\n",
    "\n",
    "        if batch_ix % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            print(f'{time_since(start, batch_ix / len(input_batches))} ({batch_ix} {batch_ix / len(input_batches) * 100:.2f}%) train_loss: {print_loss_avg:.4f}')\n",
    "    \n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_valid, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for input_batch, target_var in zip(input_batches, target_batches):\n",
    "    \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        gcn1.eval()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                     False, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "    val_loss = print_loss_total / len(input_batches)\n",
    "    validation_losses.append(val_loss)\n",
    "    # Evaluating Bleu\n",
    "    evaluator = Evaluator(encoder, decoder, gcn1, None, input_lang, output_lang, MAX_LENGTH, True)\n",
    "    candidates, references = evaluator.get_candidates_and_references(pairs_test, arr_dep_test, k_beams=1)\n",
    "    bleu = BLEU(candidates, [references])\n",
    "    validation_bleu.append(bleu)\n",
    "    print(f'val_loss: {val_loss:.4f} - bleu: {bleu}', end=' ')\n",
    "\n",
    "    # Prevent overflow gpu memory\n",
    "    del evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
