{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data import generate_batches\n",
    "from data import prepare_data\n",
    "from data import data_to_index\n",
    "from data import DEP_LABELS\n",
    "\n",
    "from model.graph import Sintactic_GCN\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder_luong\n",
    "\n",
    "from BLEU import BLEU\n",
    "\n",
    "from utils import time_since\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from validation import Evaluator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "SPLIT_TRAIN = 0.7\n",
    "SPLIT_VALID = 0.15\n",
    "# The rest is for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare vocabulary and pairs for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 118964 sentence pairs\n",
      "Filtered to 85785 pairs\n",
      "Creating vocab...\n",
      "Indexed 12436 words in input language, 22765 words in output\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('en', 'spa', max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting pairs into test, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shuffle(pairs)\n",
    "pairs_train = pairs[:int(len(pairs) * SPLIT_TRAIN)]\n",
    "pairs_valid = pairs[int(len(pairs) * SPLIT_TRAIN):int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID))]\n",
    "pairs_test = pairs[int(len(pairs) * (SPLIT_TRAIN + SPLIT_VALID)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60049, 12868, 12868)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_train), len(pairs_valid), len(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the adjacency matrix for the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'/home/krivas/stanford-corenlp-full-2018-02-27/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_adjacency_matrix(pairs):\n",
    "    arr_dep = []\n",
    "    for pair in tqdm(pairs):\n",
    "        arr_dep.append(nlp.dependency_parse(pair[0]))\n",
    "    return np.array(arr_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60049/60049 [07:22<00:00, 135.68it/s]\n",
      "100%|██████████| 12868/12868 [02:01<00:00, 106.01it/s]\n",
      "100%|██████████| 12868/12868 [02:26<00:00, 87.54it/s]\n"
     ]
    }
   ],
   "source": [
    "arr_dep_train = get_adjacency_matrix(pairs_train)\n",
    "arr_dep_valid = get_adjacency_matrix(pairs_valid)\n",
    "arr_dep_test = get_adjacency_matrix(pairs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting words to index in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train = data_to_index(pairs_train, input_lang, output_lang)\n",
    "pairs_valid = data_to_index(pairs_valid, input_lang, output_lang)\n",
    "pairs_test = data_to_index(pairs_test, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_batch_luong(batch_size, input_batches, target_batches, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "        \n",
    "    hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, hidden)\n",
    "    decoder_input = Variable(torch.LongTensor([input_lang.vocab.stoi[\"<sos>\"]] * batch_size))\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_context = Variable(torch.zeros(batch_size, decoder.hidden_size)) \n",
    "    \n",
    "    all_decoder_outputs = Variable(torch.zeros(target_batches.data.size()[0], batch_size, len(output_lang.vocab.itos)))\n",
    "\n",
    "    if USE_CUDA:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "    \n",
    "    if train:\n",
    "        use_teacher_forcing = np.random.random() < tf_ratio\n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "    \n",
    "    if use_teacher_forcing:        \n",
    "        # Use targets as inputs\n",
    "        for di in range(target_batches.shape[0]):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            decoder_input = target_batches[di]\n",
    "    else:        \n",
    "        # Use decoder output as inputs\n",
    "        for di in range(target_batches.shape[0]):            \n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input.unsqueeze(0), decoder_context, decoder_hidden, encoder_outputs) \n",
    "            \n",
    "            all_decoder_outputs[di] = decoder_output\n",
    "            \n",
    "            # Greedy approach, take the word with highest probability\n",
    "            topv, topi = decoder_output.data.topk(1)            \n",
    "            decoder_input = Variable(torch.LongTensor(topi.cpu()).squeeze())\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "        \n",
    "    del decoder_output\n",
    "    del decoder_hidden\n",
    "        \n",
    "    return all_decoder_outputs, target_batches\n",
    "\n",
    "def train_luong(input_batches, target_batches, batch_size, train=True, adj_arc_in=None, adj_arc_out=None, adj_lab_in=None, adj_lab_out=None, mask_in=None, mask_out=None, mask_loop=None):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    if train:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "    loss = 0 # Added onto for each word\n",
    "    all_decoder_outputs, target_batches = pass_batch_luong(batch_size, input_batches, target_batches, train, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "    \n",
    "    # Loss calculation and backpropagation\n",
    "    loss = criterion(all_decoder_outputs.view(-1, decoder.output_size), target_batches.contiguous().view(-1))\n",
    "    \n",
    "    if train:\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        if gcn1:\n",
    "            torch.nn.utils.clip_grad_norm_(gcn1.parameters(), clip)\n",
    "            gcn1_optimizer.step()\n",
    "\n",
    "    del all_decoder_outputs\n",
    "    del target_batches\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure models\n",
    "hidden_size_rnn = 512\n",
    "hidden_size_graph = 512\n",
    "emb_size=300\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 10.0\n",
    "learning_rate_graph = 0.0002\n",
    "n_epochs = 20\n",
    "print_every = 10\n",
    "validate_loss_every = 50\n",
    "validate_acc_every = 2 * validate_loss_every\n",
    "tf_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "encoder = Encoder(len(input_lang.vocab.itos), hidden_size_rnn, emb_size, n_layers=n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "decoder = Decoder_luong('general', hidden_size_graph, len(output_lang.vocab.itos), 300, n_layers=2 * n_layers, dropout=dropout, USE_CUDA=USE_CUDA)\n",
    "gcn1 = Sintactic_GCN(hidden_size_rnn, hidden_size_graph, num_labels=len(DEP_LABELS))\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "gcn1_optimizer = optim.Adam(gcn1.parameters(), learning_rate_graph)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    gcn1 = gcn1.cuda()\n",
    "    \n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "validation_bleu = []\n",
    "\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19m 22s (- 2308m 5s) (10 0.83%) train_loss: 1.1258\n",
      "19m 25s (- 1147m 10s) (20 1.67%) train_loss: 1.0616\n",
      "19m 28s (- 760m 7s) (30 2.50%) train_loss: 0.8672\n",
      "19m 31s (- 566m 33s) (40 3.33%) train_loss: 0.9690\n",
      "19m 33s (- 450m 22s) (50 4.16%) train_loss: 1.1023\n",
      "19m 36s (- 372m 55s) (60 5.00%) train_loss: 1.0047\n",
      "19m 39s (- 317m 30s) (70 5.83%) train_loss: 1.0243\n",
      "19m 41s (- 276m 0s) (80 6.66%) train_loss: 0.9711\n",
      "19m 44s (- 243m 43s) (90 7.49%) train_loss: 0.9698\n",
      "19m 47s (- 217m 55s) (100 8.33%) train_loss: 0.9571\n",
      "19m 50s (- 196m 46s) (110 9.16%) train_loss: 1.0412\n",
      "19m 53s (- 179m 9s) (120 9.99%) train_loss: 1.1032\n",
      "19m 56s (- 164m 14s) (130 10.82%) train_loss: 0.9723\n",
      "19m 58s (- 151m 26s) (140 11.66%) train_loss: 0.9889\n",
      "20m 1s (- 140m 20s) (150 12.49%) train_loss: 0.9124\n",
      "20m 4s (- 130m 37s) (160 13.32%) train_loss: 0.9811\n",
      "20m 7s (- 122m 2s) (170 14.15%) train_loss: 1.0140\n",
      "20m 10s (- 114m 25s) (180 14.99%) train_loss: 1.0233\n",
      "20m 13s (- 107m 35s) (190 15.82%) train_loss: 1.0557\n",
      "20m 15s (- 101m 25s) (200 16.65%) train_loss: 1.0285\n",
      "20m 18s (- 95m 51s) (210 17.49%) train_loss: 1.1190\n",
      "20m 21s (- 90m 47s) (220 18.32%) train_loss: 0.9345\n",
      "20m 24s (- 86m 9s) (230 19.15%) train_loss: 1.0155\n",
      "20m 27s (- 81m 53s) (240 19.98%) train_loss: 1.1033\n",
      "20m 29s (- 77m 58s) (250 20.82%) train_loss: 0.9873\n",
      "20m 32s (- 74m 21s) (260 21.65%) train_loss: 1.0388\n",
      "20m 35s (- 71m 1s) (270 22.48%) train_loss: 1.0054\n",
      "20m 38s (- 67m 53s) (280 23.31%) train_loss: 1.1280\n",
      "20m 41s (- 64m 58s) (290 24.15%) train_loss: 0.9778\n",
      "20m 43s (- 62m 15s) (300 24.98%) train_loss: 1.1229\n",
      "20m 46s (- 59m 43s) (310 25.81%) train_loss: 1.0004\n",
      "20m 49s (- 57m 19s) (320 26.64%) train_loss: 0.9638\n",
      "20m 52s (- 55m 4s) (330 27.48%) train_loss: 0.8964\n",
      "20m 54s (- 52m 57s) (340 28.31%) train_loss: 0.9628\n",
      "20m 57s (- 50m 58s) (350 29.14%) train_loss: 0.9774\n",
      "21m 0s (- 49m 5s) (360 29.98%) train_loss: 0.9633\n",
      "21m 3s (- 47m 17s) (370 30.81%) train_loss: 1.0015\n",
      "21m 6s (- 45m 35s) (380 31.64%) train_loss: 0.9514\n",
      "21m 8s (- 43m 58s) (390 32.47%) train_loss: 0.8944\n",
      "21m 11s (- 42m 26s) (400 33.31%) train_loss: 1.1050\n",
      "21m 14s (- 40m 57s) (410 34.14%) train_loss: 0.9906\n",
      "21m 16s (- 39m 34s) (420 34.97%) train_loss: 0.9466\n",
      "21m 19s (- 38m 14s) (430 35.80%) train_loss: 0.9278\n",
      "21m 22s (- 36m 57s) (440 36.64%) train_loss: 0.9915\n",
      "21m 25s (- 35m 44s) (450 37.47%) train_loss: 0.9499\n",
      "21m 27s (- 34m 34s) (460 38.30%) train_loss: 1.0011\n",
      "21m 30s (- 33m 27s) (470 39.13%) train_loss: 0.9546\n",
      "21m 33s (- 32m 23s) (480 39.97%) train_loss: 0.9793\n",
      "21m 36s (- 31m 21s) (490 40.80%) train_loss: 1.0740\n",
      "21m 39s (- 30m 21s) (500 41.63%) train_loss: 0.8828\n",
      "21m 42s (- 29m 24s) (510 42.46%) train_loss: 1.0281\n",
      "21m 44s (- 28m 28s) (520 43.30%) train_loss: 0.9560\n",
      "21m 47s (- 27m 35s) (530 44.13%) train_loss: 1.2080\n",
      "21m 49s (- 26m 43s) (540 44.96%) train_loss: 0.9362\n",
      "21m 52s (- 25m 53s) (550 45.80%) train_loss: 0.8837\n",
      "21m 55s (- 25m 5s) (560 46.63%) train_loss: 0.9964\n",
      "21m 58s (- 24m 19s) (570 47.46%) train_loss: 1.0381\n",
      "22m 0s (- 23m 34s) (580 48.29%) train_loss: 1.1906\n",
      "22m 3s (- 22m 50s) (590 49.13%) train_loss: 1.0595\n",
      "22m 6s (- 22m 8s) (600 49.96%) train_loss: 1.0045\n",
      "22m 9s (- 21m 27s) (610 50.79%) train_loss: 0.9800\n",
      "22m 11s (- 20m 47s) (620 51.62%) train_loss: 0.8338\n",
      "22m 14s (- 20m 9s) (630 52.46%) train_loss: 1.0822\n",
      "22m 17s (- 19m 32s) (640 53.29%) train_loss: 0.8999\n",
      "22m 20s (- 18m 56s) (650 54.12%) train_loss: 1.0875\n",
      "22m 22s (- 18m 20s) (660 54.95%) train_loss: 0.9992\n",
      "22m 25s (- 17m 46s) (670 55.79%) train_loss: 1.0041\n",
      "22m 28s (- 17m 13s) (680 56.62%) train_loss: 1.0856\n",
      "22m 31s (- 16m 40s) (690 57.45%) train_loss: 0.8495\n",
      "22m 34s (- 16m 9s) (700 58.28%) train_loss: 1.1390\n",
      "22m 37s (- 15m 38s) (710 59.12%) train_loss: 1.0099\n",
      "22m 39s (- 15m 8s) (720 59.95%) train_loss: 1.0162\n",
      "22m 42s (- 14m 39s) (730 60.78%) train_loss: 0.9113\n",
      "22m 45s (- 14m 10s) (740 61.62%) train_loss: 1.0101\n",
      "22m 48s (- 13m 42s) (750 62.45%) train_loss: 1.2286\n",
      "22m 51s (- 13m 15s) (760 63.28%) train_loss: 0.9584\n",
      "22m 54s (- 12m 49s) (770 64.11%) train_loss: 1.0153\n",
      "22m 57s (- 12m 23s) (780 64.95%) train_loss: 0.9467\n",
      "23m 0s (- 11m 58s) (790 65.78%) train_loss: 0.9502\n",
      "23m 3s (- 11m 33s) (800 66.61%) train_loss: 1.0437\n",
      "23m 6s (- 11m 9s) (810 67.44%) train_loss: 0.9912\n",
      "23m 9s (- 10m 45s) (820 68.28%) train_loss: 0.9251\n",
      "23m 11s (- 10m 22s) (830 69.11%) train_loss: 1.0497\n",
      "23m 14s (- 9m 59s) (840 69.94%) train_loss: 0.9849\n",
      "23m 17s (- 9m 37s) (850 70.77%) train_loss: 0.9604\n",
      "23m 20s (- 9m 15s) (860 71.61%) train_loss: 1.1145\n",
      "23m 23s (- 8m 53s) (870 72.44%) train_loss: 1.0210\n",
      "23m 26s (- 8m 33s) (880 73.27%) train_loss: 1.1951\n",
      "23m 29s (- 8m 12s) (890 74.10%) train_loss: 1.0027\n",
      "23m 31s (- 7m 52s) (900 74.94%) train_loss: 0.9620\n",
      "23m 34s (- 7m 32s) (910 75.77%) train_loss: 0.9965\n",
      "23m 37s (- 7m 13s) (920 76.60%) train_loss: 0.9552\n",
      "23m 41s (- 6m 54s) (930 77.44%) train_loss: 1.1201\n",
      "23m 44s (- 6m 35s) (940 78.27%) train_loss: 0.9756\n",
      "23m 46s (- 6m 17s) (950 79.10%) train_loss: 1.0793\n",
      "23m 50s (- 5m 59s) (960 79.93%) train_loss: 1.0975\n",
      "23m 53s (- 5m 41s) (970 80.77%) train_loss: 1.1089\n",
      "23m 56s (- 5m 23s) (980 81.60%) train_loss: 1.0910\n",
      "23m 58s (- 5m 6s) (990 82.43%) train_loss: 1.0207\n",
      "24m 2s (- 4m 49s) (1000 83.26%) train_loss: 0.9774\n",
      "24m 5s (- 4m 33s) (1010 84.10%) train_loss: 0.9465\n",
      "24m 7s (- 4m 16s) (1020 84.93%) train_loss: 0.9235\n",
      "24m 10s (- 4m 0s) (1030 85.76%) train_loss: 1.0900\n",
      "24m 14s (- 3m 45s) (1040 86.59%) train_loss: 1.0806\n",
      "24m 16s (- 3m 29s) (1050 87.43%) train_loss: 0.9937\n",
      "24m 19s (- 3m 14s) (1060 88.26%) train_loss: 1.0201\n",
      "24m 22s (- 2m 59s) (1070 89.09%) train_loss: 1.1079\n",
      "24m 25s (- 2m 44s) (1080 89.93%) train_loss: 0.9694\n",
      "24m 28s (- 2m 29s) (1090 90.76%) train_loss: 1.0635\n",
      "24m 31s (- 2m 15s) (1100 91.59%) train_loss: 1.2032\n",
      "24m 34s (- 2m 0s) (1110 92.42%) train_loss: 1.0674\n",
      "24m 36s (- 1m 46s) (1120 93.26%) train_loss: 1.0577\n",
      "24m 39s (- 1m 32s) (1130 94.09%) train_loss: 0.9857\n",
      "24m 42s (- 1m 19s) (1140 94.92%) train_loss: 1.0652\n",
      "24m 45s (- 1m 5s) (1150 95.75%) train_loss: 1.1350\n",
      "24m 48s (- 0m 52s) (1160 96.59%) train_loss: 1.1189\n",
      "24m 50s (- 0m 39s) (1170 97.42%) train_loss: 1.0362\n",
      "24m 53s (- 0m 26s) (1180 98.25%) train_loss: 1.0725\n",
      "24m 56s (- 0m 13s) (1190 99.08%) train_loss: 0.9721\n",
      "24m 59s (- 0m 1s) (1200 99.92%) train_loss: 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12867/12867 [05:23<00:00, 39.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 2.7739 - bleu: (0.0012412776617202854, [0.20323008945867704, 0.005821384730254749, 0.0009372354577337042, 3.7534015201276154e-05], 0.48870516398975156) 31m 2s (- 3697m 25s) (10 0.83%) train_loss: 0.5350\n",
      "31m 5s (- 1835m 37s) (20 1.67%) train_loss: 0.5244\n",
      "31m 7s (- 1215m 4s) (30 2.50%) train_loss: 0.5714\n",
      "31m 10s (- 904m 46s) (40 3.33%) train_loss: 0.5122\n",
      "31m 12s (- 718m 35s) (50 4.16%) train_loss: 0.5560\n",
      "31m 15s (- 594m 26s) (60 5.00%) train_loss: 0.5952\n",
      "31m 18s (- 505m 44s) (70 5.83%) train_loss: 0.5704\n",
      "31m 20s (- 439m 12s) (80 6.66%) train_loss: 0.5518\n",
      "31m 22s (- 387m 24s) (90 7.49%) train_loss: 0.4451\n",
      "31m 25s (- 345m 56s) (100 8.33%) train_loss: 0.5614\n",
      "31m 27s (- 312m 1s) (110 9.16%) train_loss: 0.4431\n",
      "31m 30s (- 283m 46s) (120 9.99%) train_loss: 0.5971\n",
      "31m 32s (- 259m 51s) (130 10.82%) train_loss: 0.5648\n",
      "31m 34s (- 239m 20s) (140 11.66%) train_loss: 0.5915\n",
      "31m 37s (- 221m 32s) (150 12.49%) train_loss: 0.5195\n",
      "31m 39s (- 205m 58s) (160 13.32%) train_loss: 0.5234\n",
      "31m 41s (- 192m 13s) (170 14.15%) train_loss: 0.4665\n",
      "31m 44s (- 179m 59s) (180 14.99%) train_loss: 0.5525\n",
      "31m 46s (- 169m 3s) (190 15.82%) train_loss: 0.5336\n",
      "31m 48s (- 159m 12s) (200 16.65%) train_loss: 0.5787\n",
      "31m 51s (- 150m 18s) (210 17.49%) train_loss: 0.5626\n",
      "31m 53s (- 142m 11s) (220 18.32%) train_loss: 0.4747\n",
      "31m 55s (- 134m 47s) (230 19.15%) train_loss: 0.4843\n",
      "31m 57s (- 127m 59s) (240 19.98%) train_loss: 0.5836\n",
      "32m 0s (- 121m 44s) (250 20.82%) train_loss: 0.5903\n",
      "32m 2s (- 115m 58s) (260 21.65%) train_loss: 0.5383\n",
      "32m 4s (- 110m 37s) (270 22.48%) train_loss: 0.5583\n",
      "32m 7s (- 105m 39s) (280 23.31%) train_loss: 0.5556\n",
      "32m 9s (- 101m 1s) (290 24.15%) train_loss: 0.5350\n",
      "32m 11s (- 96m 42s) (300 24.98%) train_loss: 0.5704\n",
      "32m 14s (- 92m 39s) (310 25.81%) train_loss: 0.5290\n",
      "32m 16s (- 88m 51s) (320 26.64%) train_loss: 0.5153\n",
      "32m 18s (- 85m 17s) (330 27.48%) train_loss: 0.5031\n",
      "32m 21s (- 81m 56s) (340 28.31%) train_loss: 0.5442\n",
      "32m 23s (- 78m 45s) (350 29.14%) train_loss: 0.5910\n",
      "32m 25s (- 75m 45s) (360 29.98%) train_loss: 0.5271\n",
      "32m 28s (- 72m 55s) (370 30.81%) train_loss: 0.5515\n",
      "32m 30s (- 70m 13s) (380 31.64%) train_loss: 0.5108\n",
      "32m 32s (- 67m 40s) (390 32.47%) train_loss: 0.5453\n",
      "32m 34s (- 65m 14s) (400 33.31%) train_loss: 0.6142\n",
      "32m 37s (- 62m 56s) (410 34.14%) train_loss: 0.5865\n",
      "32m 39s (- 60m 44s) (420 34.97%) train_loss: 0.6042\n",
      "32m 42s (- 58m 38s) (430 35.80%) train_loss: 0.5542\n",
      "32m 45s (- 56m 38s) (440 36.64%) train_loss: 0.5315\n",
      "32m 47s (- 54m 43s) (450 37.47%) train_loss: 0.6133\n",
      "32m 49s (- 52m 53s) (460 38.30%) train_loss: 0.6014\n",
      "32m 52s (- 51m 7s) (470 39.13%) train_loss: 0.5059\n",
      "32m 54s (- 49m 26s) (480 39.97%) train_loss: 0.4953\n",
      "32m 57s (- 47m 49s) (490 40.80%) train_loss: 0.6083\n",
      "32m 59s (- 46m 15s) (500 41.63%) train_loss: 0.6366\n",
      "33m 2s (- 44m 46s) (510 42.46%) train_loss: 0.6014\n",
      "33m 4s (- 43m 19s) (520 43.30%) train_loss: 0.6007\n",
      "33m 7s (- 41m 56s) (530 44.13%) train_loss: 0.5753\n",
      "33m 11s (- 40m 37s) (540 44.96%) train_loss: 0.5271\n",
      "33m 14s (- 39m 21s) (550 45.80%) train_loss: 0.5560\n",
      "33m 18s (- 38m 7s) (560 46.63%) train_loss: 0.5866\n",
      "33m 22s (- 36m 56s) (570 47.46%) train_loss: 0.5681\n",
      "33m 25s (- 35m 47s) (580 48.29%) train_loss: 0.6085\n",
      "33m 29s (- 34m 41s) (590 49.13%) train_loss: 0.6208\n",
      "33m 33s (- 33m 36s) (600 49.96%) train_loss: 0.5628\n",
      "33m 36s (- 32m 33s) (610 50.79%) train_loss: 0.5886\n",
      "33m 40s (- 31m 33s) (620 51.62%) train_loss: 0.5762\n",
      "33m 43s (- 30m 34s) (630 52.46%) train_loss: 0.5673\n",
      "33m 47s (- 29m 37s) (640 53.29%) train_loss: 0.4882\n",
      "33m 51s (- 28m 41s) (650 54.12%) train_loss: 0.6361\n",
      "33m 54s (- 27m 48s) (660 54.95%) train_loss: 0.5519\n",
      "33m 58s (- 26m 55s) (670 55.79%) train_loss: 0.5507\n",
      "34m 2s (- 26m 4s) (680 56.62%) train_loss: 0.5922\n",
      "34m 6s (- 25m 15s) (690 57.45%) train_loss: 0.5721\n",
      "34m 9s (- 24m 27s) (700 58.28%) train_loss: 0.6263\n",
      "34m 13s (- 23m 40s) (710 59.12%) train_loss: 0.5518\n",
      "34m 17s (- 22m 54s) (720 59.95%) train_loss: 0.5056\n",
      "34m 20s (- 22m 9s) (730 60.78%) train_loss: 0.5587\n",
      "34m 24s (- 21m 26s) (740 61.62%) train_loss: 0.6445\n",
      "34m 28s (- 20m 43s) (750 62.45%) train_loss: 0.5899\n",
      "34m 31s (- 20m 2s) (760 63.28%) train_loss: 0.5791\n",
      "34m 35s (- 19m 21s) (770 64.11%) train_loss: 0.5893\n",
      "34m 39s (- 18m 42s) (780 64.95%) train_loss: 0.5768\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs): \n",
    "    # Shuffle data\n",
    "    id_aux = np.random.permutation(np.arange(len(pairs_train)))\n",
    "    pairs_train = pairs_train[id_aux]\n",
    "    arr_dep_train = arr_dep_train[id_aux]\n",
    "    \n",
    "    # Get the batches for this epoch\n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_train, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for batch_ix, (input_batch, target_var) in enumerate(zip(input_batches, target_batches)):\n",
    "    \n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        gcn1.train()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                    True, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if batch_ix == 0: continue\n",
    "\n",
    "        if batch_ix % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            print(f'{time_since(start, batch_ix / len(input_batches))} ({batch_ix} {batch_ix / len(input_batches) * 100:.2f}%) train_loss: {print_loss_avg:.4f}')\n",
    "    \n",
    "    input_batches, target_batches = generate_batches(input_lang, output_lang, batch_size, pairs_valid, return_dep_tree=True, arr_dep=arr_dep_train, max_degree=6, USE_CUDA=USE_CUDA)\n",
    "    print_loss_total = 0\n",
    "    for input_batch, target_var in zip(input_batches, target_batches):\n",
    "    \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        gcn1.eval()\n",
    "    \n",
    "        [input_var, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop] = input_batch\n",
    "        # Run the train function\n",
    "        loss = train_luong(input_var, target_var, input_var.size(1), \n",
    "                     False, adj_arc_in, adj_arc_out, adj_lab_in, adj_lab_out, mask_in, mask_out, mask_loop)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "    val_loss = print_loss_total / len(input_batches)\n",
    "    validation_losses.append(val_loss)\n",
    "    # Evaluating Bleu\n",
    "    evaluator = Evaluator(encoder, decoder, gcn1, None, input_lang, output_lang, MAX_LENGTH, True)\n",
    "    candidates, references = evaluator.get_candidates_and_references(pairs_test, arr_dep_test, k_beams=1)\n",
    "    bleu = BLEU(candidates, [references])\n",
    "    validation_bleu.append(bleu)\n",
    "    print(f'val_loss: {val_loss:.4f} - bleu: {bleu}', end=' ')\n",
    "\n",
    "    # Prevent overflow gpu memory\n",
    "    del evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
